# Complete Integration Guide for OCR and VLM with ROS 2

This guide explains how to integrate the MobileVLM Vision-Language Model with your ROS 2 workspace, creating a complete pipeline from camera image capture to OCR text detection to VLM inference and finally to robot control actions.

## System Overview

The system consists of four main components:

1. **Camera Node**: Publishes camera images to `/front_stereo_camera/left/image_raw`
2. **OCR Node**: Reads camera images, performs OCR, and publishes text to `/ocr_text`
3. **VLM Node**: Takes both camera images and OCR text, performs inference, and publishes results to `/vlm_inference_output`
4. **Action Controller Node**: Makes decisions based on VLM inference and executes actions

## Directory Structure

Your workspace should be organized as follows:

```
~/VGN/
├── MobileVLM/            # The original MobileVLM repository
│   ├── mobilevlm/
│   ├── scripts/
│   └── ...
│
└── vgn_ros_ws/           # Your ROS 2 workspace
    └── src/
        ├── ocr_pkg/
        │   ├── CMakeLists.txt
        │   ├── package.xml
        │   └── src/
        │       └── ocr.py
        ├── vlm_pkg/
        │   ├── CMakeLists.txt
        │   ├── package.xml
        │   └── src/
        │       └── vlm.py
        └── ac_pkg/
            ├── CMakeLists.txt
            ├── package.xml
            ├── launch/
            │   └── system.launch.py
            └── src/
                └── ac.py
```

## Setup Steps

### 1. Configure Python Path

To ensure that the VLM node can access the MobileVLM repository, add the following to your `.bashrc`:

```bash
export PYTHONPATH=$PYTHONPATH:~/VGN/MobileVLM
```

Source your `.bashrc`:

```bash
source ~/.bashrc
```

### 2. Install Dependencies

```bash
# ROS 2 dependencies
sudo apt install python3-opencv python3-cv-bridge

# OCR and other Python dependencies
pip install easyocr
cd ~/VGN/MobileVLM
pip install -r requirements.txt
```

### 3. Fix Package XML Files

There seems to be an issue with rendering the XML file content. Make sure your package.xml files have the correct "name" tag format:

For ocr_pkg/package.xml, make sure the name tag looks like:
```xml
<name>ocr_pkg</name>
```

Instead of:
```xml
<n>ocr_pkg</n>
```

Do the same correction for vlm_pkg/package.xml.

### 4. Build the Workspace

```bash
cd ~/VGN/vgn_ros_ws
colcon build
source install/setup.bash
```

### 5. Run the System

You can start each node individually:

```bash
# Terminal 1
ros2 run ocr_pkg ocr.py

# Terminal 2
ros2 run vlm_pkg vlm.py

# Terminal 3
ros2 run ac_pkg ac.py
```

Alternatively, use the launch file:

```bash
ros2 launch ac_pkg system.launch.py
```

## Testing the System

To verify the system is working correctly:

1. Check if the nodes are running:
   ```bash
   ros2 node list
   ```

2. Check available topics:
   ```bash
   ros2 topic list
   ```

3. Monitor OCR output:
   ```bash
   ros2 topic echo /ocr_text
   ```

4. Monitor VLM inference output:
   ```bash
   ros2 topic echo /vlm_inference_output
   ```

## Troubleshooting

### MobileVLM Import Issues

If you encounter import errors related to MobileVLM:

1. Check that the path in the VLM node is correct:
   ```python
   MOBILEVLM_PATH = os.path.expanduser("~/VGN/MobileVLM")
   ```

2. Ensure MobileVLM repository is properly installed:
   ```bash
   cd ~/VGN/MobileVLM
   pip install -e .  # Install in development mode
   ```

### OCR or VLM Performance Issues

- If OCR is processing too slowly, adjust the `process_interval` parameter in the OCR node
- If VLM inference is taking too long, consider reducing the `max_new_tokens` parameter in the VLM node

### ROS 2 Communication Issues

- Check that topics are being published:
  ```bash
  ros2 topic info /ocr_text
  ros2 topic info /vlm_inference_output
  ```

- Ensure all nodes are in the same ROS domain:
  ```bash
  echo $ROS_DOMAIN_ID
  ```

## Customizing the System

### Modifying OCR Behavior

The OCR node processes frames at a fixed interval (10 seconds by default). To change this:

1. Edit the `process_interval` parameter in `ocr.py`:
   ```python
   self.process_interval = 5.0  # Process every 5 seconds
   ```

### Modifying VLM Behavior

To change VLM model or inference parameters:

1. Edit the model path in `vlm.py`:
   ```python
   self.model_path = "mtgv/MobileVLM-1.7B"  # Use original MobileVLM instead of V2
   ```

2. Modify inference parameters:
   ```python
   args = type('Args', (), {
       # ...
       "temperature": 0.7,  # Add some randomness to responses
       "max_new_tokens": 256,  # Shorter responses
       # ...
   })()
   ```

### Adding New Actions to the Controller

To add new actions based on VLM output:

1. Add new condition in the `vlm_callback` method in `ac.py`:
   ```python
   elif "turn right" in inference_output.lower():
       self.execute_right_turn()
   ```

2. Add the corresponding method:
   ```python
   def execute_right_turn(self):
       self.get_logger().info("Executing right turn")
       # Implementation details
   ```

## Next Steps

1. **Error Handling**: Add more robust error handling for cases where OCR detects nothing or VLM inference fails
2. **Visualization**: Consider adding a visualization node to display the OCR results and VLM outputs
3. **Feedback Loop**: Add a feedback mechanism where the robot's actions affect what the camera sees next
4. **Multiple Models**: Experiment with different VLM models for different tasks
5. **Fine-tuning**: Fine-tune the MobileVLM model on your specific task
