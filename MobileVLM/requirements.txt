torch==2.6.0                   # CUDA 12.4-compatible (default wheel works with system CUDA)
torchvision==0.21.0            # Matching torchvision
deepspeed==0.14.2              # Updated for torch 2.6.0
transformers==4.39.3           # Compatible with updated torch
tokenizers==0.15.2
sentencepiece==0.1.99
shortuuid==1.0.11
accelerate==0.30.1             # Compatible with newer transformers
peft==0.10.0                   # Needed for transformers >= 4.39
bitsandbytes==0.43.0           # Works with CUDA 12.4
pydantic==1.10.13
markdown2==2.4.8
numpy==1.25.0
scikit-learn==1.2.2
gradio==3.35.2
requests==2.28.2
httpx==0.24.0
uvicorn==0.22.0
fastapi==0.103.0
einops==0.8.1                  # Needed for flash-attn >= 2.7
einops-exts==0.0.4
timm==0.9.12
triton==3.2.0                  # Required for flash-attn

# Flash-Attn built from source (for CUDA 12.4 compatibility)
flash-attn @ git+https://github.com/Dao-AILab/flash-attention.git

